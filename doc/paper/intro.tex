\section{Introduction}
\label{chap:intro}

Hash tables are heavily-used data structures in servers. This paper focuses on fixed-size hash tables that support eviction of its elements using a Least Recently Used (LRU) list. Such hash tables are a good way to implement a key/value cache. A popular distributed applications that uses a key/value cache is \memcached{} \cite{memcached}. \memcached{} is an in-memory cache for Web applications that store data, page rendering results, and other information that can be cached and is expensive to recalculate.

This paper explores the use of message passing to increase scalability and performance of a hash table on multi-core processors.  In a multi-core processor, each core has its own cache and perhaps a few shared caches.  The cache-coherence protocol transfers cache lines between caches to ensure memory coherence.  Fetching lines from memory or from other cores's caches is expensive, varying from one order to two order of magnitude in latency, compared to an L1 fetch.

This paper introduces a new hash table, which we call \cphash{}, which uses message passing to reduce cache-line transfers between cores and increase performance and scalability by reducing total number of cache misses. Instead of having each core access any part of a hash table, \cphash{} splits the hash table into partitions and assign a partition to a particular core. \cphash{} uses message passing to pass the lookup/insert operation to the core that is assigned the partition needed for that particular operation. \cphash{} assumes that message passing will work well when modifiable data contents are large and the computation description is small. This paper strives to prove this assumption by providing an implementation of \cphash{} and demonstrating its performance gains.

Experiments on a 48 \XXX{160?} -core machine with \cphash{} and an implementation of a hash table with fine-grained locks show that \cphash{} achieves better scalability and performance due to two reasons: Decrease in cache capacity misses (for small data sets) and decrease in cache coherency misses (for all data sets).  Cache capacity misses are reduced since \cphash{} avoids data transfers and thus data duplication in different caches. Cache coherency misses are reduced due to caching common partition data that are modified, which in \cphash{} is the head of the LRU list. Both the lookup and the insert operations access and modify the head of the LRU list.

To investigate if these benefits are material to real applications, this paper introduces a memcached-style key/value cache server, which we call \cpserver{}, which uses \cphash{} as its hash table.  We compare the performance of \cpserver{} to the performance of a key/value cache server that uses a hash table with fine-grained locks. We also compare the performance of \cpserver{} against \memcached{}.  We observe increase in throughput in both cases due to the speedup provided by the \cphash{} hash table implementation.  \XXX{how much improvement}

The main contributions of the paper as follows. \XXX{fill out}

The remainder of this paper is structured as follows. Section \ref{sec:related} describes the related work. Section \ref{sec:design} describes \cphash{}'s design. Section~\ref{sec:mm} describes \cphash{}'s memory management algorithm for allocating and storing the hash table contents in memory. Section~\ref{sec:server} describes design and protocol of \cpserver{}.  Section~\ref{sec:eval} describes the benchmarking methods and contains a detailed evaluation of the performance gains. In Section~\ref{sec:future} we discuss future plans for \cphash{}.  Section~\ref{sec:concl} summarizes our conclusions.

